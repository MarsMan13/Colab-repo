{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"7_CNN_2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO0DU3Ug8TAltaOuuIaR4Ww"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"zgsk8FCnhgod"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["from sklearn.datasets import fetch_openml\n","\n","mnist = fetch_openml('mnist_784')"],"metadata":{"id":"kOJx6gp0LdWu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","mnist_data, mnist_target = mnist.data.values, mnist.target.values\n","X, y = StandardScaler().fit_transform(mnist_data), mnist_target.astype(np.int)\n","y_ = np.zeros((y.size, 10))\n","y_[np.arange(y.size), y] = 1"],"metadata":{"id":"SAlPAZSymPtB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Implementing all layers of CNN totally"],"metadata":{"id":"VmTgjK50DjWW"}},{"cell_type":"markdown","source":["### image to col & col to image function"],"metadata":{"id":"yAGTLA5tEVK9"}},{"cell_type":"code","source":["def im2col(input_data, FH, FW, stride=1, pad=0):\n","  \"\"\"\n","  input_data : (N, C, H, W)\n","  fh, fw : int\n","  stride, pad : int\n","  \"\"\"\n","  N, C, H, W = input_data.shape\n","  OH = (H + 2 * pad - FH) // stride + 1\n","  OW = (W + 2 * pad - FW) // stride + 1\n","  img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n","  col = np.zeros((N, OH, OW, C, FH, FW))\n","\n","  for h in range(OH):\n","    ii = h * stride\n","    for w in range(OW):\n","      jj = w * stride\n","      col[:, h, w, :, :, :] = img[:, :, ii:ii+FH, jj:jj+FW]\n","  col = col.reshape(N*OH*OW, -1)\n","  return col\n","\n","def col2im(col, input_shape, FH, FW, stride=1, pad=0):\n","  N, C, H, W = input_shape\n","  OH = (H + 2 * pad - FH) // stride + 1\n","  OW = (W + 2 * pad - FW) // stride + 1\n","  col = col.reshape(N, OH, OW, C, FH, FW)\n","  img = np.zeros((N, C, H+2*pad, W+2*pad))\n","  for h in range(OH):\n","    ii = h * stride\n","    for w in range(OW):\n","      jj = w * stride\n","      img[:,:,ii:ii+FH, jj:jj+FW] = col[:, h, w, :, :, :]\n","  return img[:, :, pad:pad+H, pad:pad+W]"],"metadata":{"id":"5LgA73Bihlyn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Activation and Evaluating function"],"metadata":{"id":"KFDIVp3sEbC9"}},{"cell_type":"code","source":["def sigmoid(x):\n","  return 1 / (1 + np.exp(-x))\n","\n","class Relu:\n","  def __init__(self):\n","    self.mask = None\n","\n","  def forward(self, X):\n","    self.mask = (X <= 0)\n","    self.out = X.copy()\n","    self.out[self.mask] = 0\n","    return self.out\n","  \n","  def backward(self, dout):\n","    dout[self.mask] = 0\n","    dx = dout\n","    return dx\n","\n","def softmax(x):\n","  C = np.max(x)\n","  exps = np.exp(x - C)\n","  if x.ndim == 1:\n","    sum_exps = np.sum(exps)\n","    return exps / sum_exps\n","  sum_exps = np.sum(exps, axis=1)\n","  return (exps.T / sum_exps).T\n","\n","def cross_entropy_error(y, t):\n","  if y.ndim == 1:\n","    y = y.reshape(1, y.size)\n","    t = t.reshape(1, t.size)\n","  batch_size = t.shape[0]\n","  return -np.sum( t * np.log(y + 1e-4) ) / batch_size"],"metadata":{"id":"noLC-gm7LeYN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Affine function"],"metadata":{"id":"kyNudV6OEyMk"}},{"cell_type":"code","source":["class Affine:\n","  def __init__(self, W, b):\n","    self.W = W\n","    self.b = b\n","    self.x = None\n","    self.original_x_shape = None\n","    self.dW = None\n","    self.db = None\n","\n","  def forward(self, x):\n","    self.original_x_shape = x.shape\n","    self.x = x.reshape(x.shape[0], -1)\n","    return np.dot(self.x, self.W) + self.b\n","  \n","  def backward(self, dout):\n","    self.db = np.sum(dout, axis=0)\n","    self.dw = np.dot(self.x.T, dout)\n","    dx = np.dot(dout, self.W.T)\n","    return dx.reshape(self.original_x_shape)"],"metadata":{"id":"w5QDYthKrDO5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### SoftmaxWithLoss function"],"metadata":{"id":"pnAb61hWE0Iz"}},{"cell_type":"code","source":["class SoftmaxWithLoss:\n","  def __init__(self):\n","    self.loss = None\n","    self.y = None\n","    self.t = None\n","  \n","  def forward(self, x, t):\n","    self.t = t\n","    self.y = softmax(x)\n","    self.loss = cross_entropy_error(self.y, self.t)\n","    return self.loss\n","  \n","  def backward(self, dout=1):\n","    batch_size = self.t.shape[0]\n","    dx = (self.y - self.t) / batch_size\n","    return dx"],"metadata":{"id":"uKYuFsQQEWP-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Initor function"],"metadata":{"id":"KX0my-QKE288"}},{"cell_type":"code","source":["def gaussianInit(input_size, output_size, weight_init_std=0.01):\n","  return np.random.randn(input_size, output_size) * weight_init_std\n","\n","def xavierInit(input_size, output_size, weight_init_std=0.01):\n","  return np.random.randn(input_size, output_size) / np.sqrt(input_size)\n","\n","def heInit(input_size, output_size, weight_init_std=0.01):\n","  return np.random.randn(input_size, output_size) * np.sqrt( 2 / input_size)"],"metadata":{"id":"lCBBstwNoazb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ANN based TwoLayerNetwork"],"metadata":{"id":"X2BUb1RSE5Aj"}},{"cell_type":"code","source":["from collections import OrderedDict\n","\n","class TwoLayerNetwork:\n","  def __init__(self, layer_num, layer_size=None, initor=\"Gaussian\", weight_init_std=0.01):\n","    self.layer_num = layer_num\n","    self.layer_size = layer_size\n","    self.params = {}\n","    init_functions = {'Gaussian':gaussianInit, 'Xavier':xavierInit, 'He':heInit}\n","    for i in range(1, self.layer_num+1):\n","      self.params['W'+str(i)] = init_functions[initor](self.layer_size[i-1], self.layer_size[i], weight_init_std)\n","      self.params['b'+str(i)] = np.zeros(self.layer_size[i])\n","    self.layers = OrderedDict()\n","    for i in range(1, self.layer_num+1):\n","      self.layers['Affine'+str(i)] = Affine(self.params['W'+str(i)], self.params['b'+str(i)])\n","      self.layers['Relu'+str(i)] = Relu()\n","    self.lastLayer = SoftmaxWithLoss()\n","  \n","  def predict(self, x):\n","    for layer in self.layers.values():\n","      x = layer.forward(x)\n","    return x\n","  \n","  def loss(self, x, t):\n","    x = self.predict(x)\n","    return self.lastLayer.forward(x, t)\n","\n","  def accuracy(self, x, t):\n","    ret = self.predict(x)\n","    y = np.argmax(ret, axis=1)\n","    t = np.argmax(t, axis=1)\n","    return np.sum(y == t) / x.shape[0]\n","\n","  def gradient(self, x, t):\n","    self.loss(x, t)\n","\n","    dout = 1\n","    dout = self.lastLayer.backward(dout)\n","    layers = list(self.layers.values())\n","    layers.reverse()\n","    for layer in layers:\n","      dout = layer.backward(dout)\n","    grads = {}\n","    for i in range(1, self.layer_num+1):\n","      grads['W'+str(i)] = self.layers['Affine'+str(i)].dw\n","      grads['b'+str(i)] = self.layers['Affine'+str(i)].db\n","    return grads"],"metadata":{"id":"s-wVQ1mPmLI5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y_, test_size=.1, stratify=y_)"],"metadata":{"id":"P3klIhVUwzSe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["layer_num = 2\n","layer_size = [784, 50, 10]\n","net = TwoLayerNetwork(layer_num, layer_size, initor=\"He\")\n","iter_num = 1000\n","lr = 0.005\n","\n","batch_size = 200\n","epoch_size = max((iter_num // batch_size), 1)\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","for i in range(iter_num):\n","  targets = np.random.choice(X_train.shape[0], batch_size)\n","  X_batch, t_batch = X_train[targets], y_train[targets]\n","\n","  grads = net.gradient(X_batch, t_batch)\n","  for ii in range(1, net.layer_num+1):\n","    net.params['W'+str(ii)] -= lr * grads['W'+str(ii)]\n","    net.params['b'+str(ii)] -= lr * grads['b'+str(ii)]\n","    loss = net.loss(X_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","  if i % epoch_size == 0:\n","    train_acc = net.accuracy(X_train, y_train)\n","    test_acc = net.accuracy(X_test, y_test)\n","    train_acc_list.append(train_acc)\n","    test_acc_list.append(test_acc)\n","    print(train_acc, test_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lSU1GpOkslPL","executionInfo":{"status":"ok","timestamp":1641100229904,"user_tz":-540,"elapsed":99662,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}},"outputId":"1990ae4f-0604-4982-e274-ba3171a630f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.10931746031746031 0.10371428571428572\n","0.1357936507936508 0.13157142857142856\n","0.16295238095238096 0.15957142857142856\n","0.19341269841269842 0.18942857142857142\n","0.22011111111111112 0.21871428571428572\n","0.24761904761904763 0.24428571428571427\n","0.2746984126984127 0.2712857142857143\n","0.2986190476190476 0.29942857142857143\n","0.3248412698412698 0.32485714285714284\n","0.34844444444444445 0.3477142857142857\n","0.36652380952380953 0.3668571428571429\n","0.38476190476190475 0.38642857142857145\n","0.4022857142857143 0.4034285714285714\n","0.4186666666666667 0.41942857142857143\n","0.4330952380952381 0.43614285714285717\n","0.44926984126984126 0.45171428571428573\n","0.46506349206349207 0.4654285714285714\n","0.4805714285714286 0.4817142857142857\n","0.49557142857142855 0.49585714285714283\n","0.5089523809523809 0.5085714285714286\n","0.5231587301587302 0.5222857142857142\n","0.5360952380952381 0.5342857142857143\n","0.5478253968253968 0.5445714285714286\n","0.558968253968254 0.5557142857142857\n","0.5695714285714286 0.5674285714285714\n","0.5800634920634921 0.5771428571428572\n","0.590031746031746 0.5868571428571429\n","0.5991746031746031 0.596\n","0.6071587301587301 0.6047142857142858\n","0.6159206349206349 0.6124285714285714\n","0.6244920634920635 0.6198571428571429\n","0.6321428571428571 0.6271428571428571\n","0.6389365079365079 0.6348571428571429\n","0.6456190476190476 0.6431428571428571\n","0.6523174603174603 0.6515714285714286\n","0.6587936507936508 0.6601428571428571\n","0.664968253968254 0.6655714285714286\n","0.6705238095238095 0.6708571428571428\n","0.6767777777777778 0.6768571428571428\n","0.6826507936507936 0.6815714285714286\n","0.6872698412698413 0.6874285714285714\n","0.6926507936507936 0.6922857142857143\n","0.6976031746031746 0.6974285714285714\n","0.7021269841269842 0.7031428571428572\n","0.7068730158730159 0.708\n","0.7111111111111111 0.7138571428571429\n","0.7163333333333334 0.7188571428571429\n","0.7200634920634921 0.7231428571428572\n","0.724015873015873 0.7278571428571429\n","0.7276984126984127 0.7308571428571429\n","0.7314444444444445 0.7325714285714285\n","0.7354603174603175 0.736\n","0.7393174603174603 0.7392857142857143\n","0.7424920634920635 0.7434285714285714\n","0.7451904761904762 0.7457142857142857\n","0.7483174603174603 0.7485714285714286\n","0.7515396825396825 0.7518571428571429\n","0.7544920634920635 0.7548571428571429\n","0.7575714285714286 0.758\n","0.7599206349206349 0.7598571428571429\n","0.7628253968253969 0.7632857142857142\n","0.7656507936507937 0.766\n","0.7680317460317461 0.7675714285714286\n","0.7709047619047619 0.7715714285714286\n","0.772984126984127 0.7737142857142857\n","0.775 0.7762857142857142\n","0.777 0.7784285714285715\n","0.7791904761904762 0.78\n","0.7812698412698412 0.7838571428571428\n","0.7832063492063492 0.7857142857142857\n","0.7848888888888889 0.7871428571428571\n","0.786952380952381 0.7898571428571428\n","0.7887301587301587 0.7911428571428571\n","0.7903650793650794 0.7918571428571428\n","0.7919206349206349 0.7934285714285715\n","0.7932698412698412 0.7947142857142857\n","0.7954444444444444 0.7964285714285714\n","0.7969206349206349 0.7974285714285714\n","0.7979047619047619 0.7985714285714286\n","0.7995396825396826 0.7994285714285714\n","0.801063492063492 0.801\n","0.8022857142857143 0.8021428571428572\n","0.8038253968253968 0.8037142857142857\n","0.8051111111111111 0.8057142857142857\n","0.806063492063492 0.8065714285714286\n","0.8074920634920635 0.8082857142857143\n","0.8086984126984127 0.8095714285714286\n","0.810047619047619 0.8102857142857143\n","0.8109206349206349 0.811\n","0.8120793650793651 0.8115714285714286\n","0.8133174603174603 0.8118571428571428\n","0.8142380952380952 0.8128571428571428\n","0.8153809523809524 0.8142857142857143\n","0.8167936507936507 0.8151428571428572\n","0.8176349206349206 0.8162857142857143\n","0.8187301587301588 0.8168571428571428\n","0.8196190476190476 0.8174285714285714\n","0.8206984126984127 0.8192857142857143\n","0.8216349206349206 0.8202857142857143\n","0.8227142857142857 0.8208571428571428\n","0.8237619047619048 0.8215714285714286\n","0.8246031746031746 0.8218571428571428\n","0.8253809523809523 0.8227142857142857\n","0.8262539682539682 0.8241428571428572\n","0.8273809523809523 0.8252857142857143\n","0.8284126984126984 0.8274285714285714\n","0.8291746031746031 0.8277142857142857\n","0.8300952380952381 0.8288571428571428\n","0.8308888888888889 0.8304285714285714\n","0.8318253968253968 0.8315714285714285\n","0.8325714285714285 0.8331428571428572\n","0.8333650793650794 0.8332857142857143\n","0.834047619047619 0.8345714285714285\n","0.8348888888888889 0.8351428571428572\n","0.8356349206349206 0.8348571428571429\n","0.8365079365079365 0.836\n","0.8373650793650793 0.8371428571428572\n","0.838 0.8374285714285714\n","0.8387142857142857 0.8378571428571429\n","0.8394444444444444 0.8395714285714285\n","0.8401269841269842 0.8402857142857143\n","0.8407301587301588 0.8405714285714285\n","0.8414603174603175 0.8412857142857143\n","0.8421746031746031 0.8417142857142857\n","0.843015873015873 0.8424285714285714\n","0.8437619047619047 0.8431428571428572\n","0.8445079365079365 0.8438571428571429\n","0.8452857142857143 0.8447142857142858\n","0.846 0.8451428571428572\n","0.8463333333333334 0.8455714285714285\n","0.8471111111111111 0.8467142857142858\n","0.8478253968253968 0.8464285714285714\n","0.8484761904761905 0.8471428571428572\n","0.8488253968253968 0.848\n","0.8493968253968254 0.8482857142857143\n","0.850015873015873 0.8488571428571429\n","0.8504285714285714 0.8495714285714285\n","0.8509523809523809 0.8501428571428571\n","0.8514285714285714 0.8511428571428571\n","0.8521111111111112 0.8511428571428571\n","0.8524920634920635 0.852\n","0.8530634920634921 0.8525714285714285\n","0.8534285714285714 0.8537142857142858\n","0.8540634920634921 0.8535714285714285\n","0.8544603174603175 0.854\n","0.8551904761904762 0.8544285714285714\n","0.8555238095238096 0.855\n","0.8560634920634921 0.855\n","0.8566825396825397 0.855\n","0.857015873015873 0.8557142857142858\n","0.8573492063492063 0.8561428571428571\n","0.858 0.8565714285714285\n","0.8582063492063492 0.8564285714285714\n","0.8587301587301587 0.8565714285714285\n","0.8595555555555555 0.8571428571428571\n","0.8599682539682539 0.8577142857142858\n","0.8603174603174604 0.8578571428571429\n","0.8605714285714285 0.8582857142857143\n","0.8608888888888889 0.859\n","0.8612063492063492 0.8592857142857143\n","0.8615873015873016 0.8594285714285714\n","0.8619523809523809 0.859\n","0.8623174603174604 0.86\n","0.8626190476190476 0.8597142857142858\n","0.8630634920634921 0.86\n","0.8636031746031746 0.8597142857142858\n","0.864015873015873 0.8605714285714285\n","0.8643968253968254 0.8618571428571429\n","0.8649365079365079 0.862\n","0.8653650793650793 0.8622857142857143\n","0.8656190476190476 0.8631428571428571\n","0.8662698412698413 0.8628571428571429\n","0.8663650793650793 0.8638571428571429\n","0.8667777777777778 0.864\n","0.8671904761904762 0.8647142857142858\n","0.8674444444444445 0.8647142857142858\n","0.8677460317460317 0.8651428571428571\n","0.8684126984126984 0.8652857142857143\n","0.8688571428571429 0.8654285714285714\n","0.8691111111111111 0.8658571428571429\n","0.8696507936507937 0.8665714285714285\n","0.8698888888888889 0.8667142857142857\n","0.8700634920634921 0.8671428571428571\n","0.8704444444444445 0.8671428571428571\n","0.8709206349206349 0.8678571428571429\n","0.8712380952380953 0.8681428571428571\n","0.8714285714285714 0.868\n","0.8717619047619047 0.8678571428571429\n","0.8720793650793651 0.8692857142857143\n","0.8723809523809524 0.8692857142857143\n","0.8726666666666667 0.87\n","0.872968253968254 0.87\n","0.8732380952380953 0.8708571428571429\n","0.8735238095238095 0.871\n","0.873984126984127 0.8712857142857143\n","0.8743333333333333 0.8717142857142857\n","0.8744444444444445 0.8722857142857143\n","0.8748253968253968 0.872\n","0.8750476190476191 0.8725714285714286\n","0.8750476190476191 0.8725714285714286\n"]}]},{"cell_type":"markdown","source":["### Convolution layer"],"metadata":{"id":"B87aEhEfE_38"}},{"cell_type":"code","source":["class Convolution:\n","  def __init__(self, W, b, stride=1, pad=0):\n","    self.W = W\n","    self.b = b\n","    self.stride = stride\n","    self.pad = pad\n","\n","  def forward(self, x):\n","    self.x = x\n","    N, C, H, W = self.x.shape\n","    FN, C, FH, FW = self.W.shape\n","    out_h = (H + 2*self.pad - FH) // self.stride + 1\n","    out_w = (W + 2*self.pad - FW) // self.stride + 1\n","    \n","    self.col = im2col(x, FH, FW, self.stride, self.pad)\n","    self.col_W = self.W.reshape(FN, -1).T\n","    out = np.dot(self.col, self.col_W) + self.b\n","\n","    out = out.reshape(N, out_h, out_w, FN).transpose(0,3,1,2)\n","    return out\n","  \n","  def backward(self, dout):\n","    N, C, H, W = self.x.shape\n","    FN, C, FH, FW = self.W.shape\n","    out_h = (H + 2*self.pad - FH) // self.stride + 1\n","    out_w = (W + 2*self.pad - FW) // self.stride + 1\n","    # step1\n","    dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n","    # step2 for db\n","    self.db = np.sum(dout, axis=0)\n","    # step3 for self.col * self.col_W\n","    dcol = np.dot(dout, self.col_W.T)\n","    dcol_W = np.dot(self.col.T, dout)\n","    # step4 for dx, dw\n","    dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n","    self.dw = dcol_W.T.reshape(FN, C, FH, FW)\n","    return dx"],"metadata":{"id":"mgRqF4b00K9n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Pooling layer"],"metadata":{"id":"LHuDfWWnMSKs"}},{"cell_type":"code","source":["class Pooling:\n","  def __init__(self, pool_h, pool_w, stride=1, pad=0):\n","    self.pool_h = pool_h\n","    self.pool_w = pool_w\n","    self.stride = stride\n","    self.pad = pad\n","  \n","  def forward(self, x):\n","    N, C, H, W = x.shape\n","    out_h = (H - self.pool_h) // self.stride + 1\n","    out_w = (W - self.pool_w) // self.stride + 1\n","    col = im2col(x, self.pool_h, self.pool_w, stride=self.stride, pad=self.pad)\n","    col = col.reshape(-1, self.pool_h * self.pool_w)\n","    self.arg_max = np.argmax(col, axis=1)\n","    out = np.max(col, axis=1)\n","    out = out.reshape(N, out_h, out_w, C).transpose(0,3,1,2)\n","    self.x = x\n","    return out\n","\n","  def backward(self, dout):\n","    N, C, H, W = self.x.shape\n","    out_h = (H - self.pool_h) // self.stride + 1\n","    out_w = (W - self.pool_w) // self.stride + 1\n","    dout = dout.transpose(0,2,3,1).reshape(N, out_h, out_w, C)\n","\n","    dmax = np.zeros((dout.size, self.pool_h * self.pool_w))\n","    dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n","    dmax = dmax.reshape(dout.shape + (self.pool_h * self.pool_w,))\n","    dcol = dmax.reshape(dmax.shape[0], dmax.shape[1], dmax.shape[2], -1)\n","    dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n","    return dx"],"metadata":{"id":"dmc3lmAuKBdy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Making CNN architecture\n","\n","<Architecture>\n","input -- --> Conv -> ReLU -> Pooling -- --> Affine -> ReLU -- --> Affine -> Softmax"],"metadata":{"id":"H5DaRwHhYuSG"}},{"cell_type":"code","source":["from collections import OrderedDict\n","\n","class SimpleConvNet:\n","  def __init__(self, input_dim=(1, 28, 28), conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}, \\\n","               hidden_size=100, output_size=10, initor=\"Gaussian\", weight_init_std=0.01):\n","    \n","    filter_num = conv_param['filter_num']\n","    filter_size = conv_param['filter_size']\n","    filter_pad = conv_param['pad']\n","    filter_stride = conv_param['stride']\n","    input_size = input_dim[1]\n","    conv_output_size = (input_size + 2 * filter_pad - filter_size) // filter_stride + 1\n","    pool_output_size = int(filter_num * (conv_output_size / 2)**2)  # pooling layer's size: 2, stride:2\n","\n","    init_functions = {'Gaussin':gaussianInit, 'Xavier':xavierInit, 'He':heInit}\n","    self.params = {}\n","    # for Convolution layer\n","    self.params['W1'] = np.random.randn(filter_num, input_dim[0], filter_size, filter_size) * weight_init_std\n","    self.params['b1'] = np.zeros(filter_num)\n","    # for Affine layer 1\n","    self.params['W2'] = init_functions[initor](pool_output_size, hidden_size, weight_init_std)\n","    self.params['b2'] = np.zeros(hidden_size)\n","    # for Affine layer 2\n","    self.params['W3'] = init_functions[initor](hidden_size, output_size, weight_init_std)\n","    self.params['b3'] = np.zeros(output_size)\n","\n","    self.layers = OrderedDict()\n","    self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'], stride=filter_stride, pad=filter_pad)\n","    self.layers['Relu1'] = Relu()\n","    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n","    self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n","    self.layers['Relu2'] = Relu()\n","    self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n","    self.last_layer = SoftmaxWithLoss()\n","  \n","  def predict(self, x):\n","    for layer in self.layers.values():\n","      x = layer.forward(x)\n","    return x\n","  \n","  def loss(self, x, t):\n","    y = self.predict(x)\n","    return self.last_layer.forward(y, t)\n","  \n","  def accuracy(self, x, t):\n","    y = self.predict(x)\n","    y = np.argmax(y, axis=1)\n","    if t.ndim != 1 : t = np.argmax(t, axis=1)\n","    accuracy = np.sum(y == t) / float(x.shape[0])\n","    return accuracy\n","\n","  def gradient(self, x, t):\n","    self.loss(x, t)\n","\n","    dout = 1\n","    dout = self.last_layer.backward(dout)\n","    layers = list(self.layers.values())\n","    layers.reverse()\n","    for layer in layers:\n","      dout = layer.backward(dout)\n","    \n","    grads = {}\n","    grads['W1'] = self.layers['Conv1'].dw\n","    grads['b1'] = self.layers['Conv1'].db\n","    grads['W2'] = self.layers['Affine1'].dw\n","    grads['b2'] = self.layers['Affine1'].db\n","    grads['W3'] = self.layers['Affine2'].dw\n","    grads['b3'] = self.layers['Affine2'].db\n","\n","    return grads"],"metadata":{"id":"pk6DBxSITcqV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_ = X.reshape(-1, 1, 28, 28)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=.1, stratify=y_)"],"metadata":{"id":"uAN9KMf7k5oA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["net = SimpleConvNet(initor=\"He\")\n","iter_num = 10000\n","lr = 0.01\n","\n","batch_size = 100\n","epoch_size = max((iter_num // batch_size), 1)\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","for i in range(iter_num):\n","  targets = np.random.choice(X_train.shape[0], batch_size)\n","  X_batch, t_batch = X_train[targets], y_train[targets]\n","\n","  grads = net.gradient(X_batch, t_batch)\n","  for ii in range(1, 3+1):\n","    net.params['W'+str(ii)] -= lr * grads['W'+str(ii)]\n","    net.params['b'+str(ii)] -= lr * grads['b'+str(ii)]\n","  loss = net.loss(X_batch, t_batch)\n","  train_loss_list.append(loss)\n","\n","  if i % epoch_size == 0:\n","    #train_acc = net.accuracy(X_traiㅑn, y_train)\n","    test_acc = net.accuracy(X_test, y_test)\n","    #train_acc_list.append(train_acc)\n","    #test_acc_list.append(test_acc)\n","    #print(train_acc, test_acc)\n","    print(test_acc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Llbtu0EEjZI1","outputId":"1165ff02-70f6-4fde-adb7-14c19e7b9651"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.10957142857142857\n","0.8172857142857143\n","0.8917142857142857\n","0.913\n","0.9248571428571428\n","0.9325714285714286\n","0.9361428571428572\n","0.9404285714285714\n","0.9447142857142857\n","0.948\n","0.9485714285714286\n","0.9532857142857143\n","0.9548571428571428\n","0.9555714285714285\n","0.9581428571428572\n","0.958\n","0.9598571428571429\n","0.9618571428571429\n","0.9625714285714285\n","0.9641428571428572\n","0.9642857142857143\n","0.9668571428571429\n","0.9671428571428572\n","0.9661428571428572\n","0.9705714285714285\n","0.968\n","0.9695714285714285\n","0.9695714285714285\n","0.969\n","0.9705714285714285\n","0.9732857142857143\n","0.9702857142857143\n","0.9718571428571429\n","0.9728571428571429\n","0.9721428571428572\n","0.9722857142857143\n","0.9734285714285714\n","0.9735714285714285\n","0.9732857142857143\n","0.9741428571428571\n","0.9747142857142858\n","0.9758571428571429\n","0.9745714285714285\n","0.9754285714285714\n","0.9758571428571429\n","0.9732857142857143\n","0.9757142857142858\n","0.9765714285714285\n","0.9765714285714285\n","0.9772857142857143\n","0.9778571428571429\n","0.9767142857142858\n","0.977\n","0.9778571428571429\n","0.9791428571428571\n","0.9777142857142858\n","0.98\n","0.9788571428571429\n","0.9785714285714285\n","0.9788571428571429\n","0.9782857142857143\n","0.9788571428571429\n","0.9797142857142858\n","0.9807142857142858\n","0.9808571428571429\n","0.9811428571428571\n","0.979\n","0.9788571428571429\n","0.9797142857142858\n","0.9787142857142858\n","0.9807142857142858\n","0.98\n","0.98\n","0.9804285714285714\n","0.981\n","0.9795714285714285\n","0.9802857142857143\n","0.9811428571428571\n","0.9817142857142858\n","0.9807142857142858\n","0.9805714285714285\n","0.9814285714285714\n","0.982\n","0.9828571428571429\n","0.9812857142857143\n","0.9817142857142858\n","0.9815714285714285\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"DFjl5GsJCnBo"},"execution_count":null,"outputs":[]}]}