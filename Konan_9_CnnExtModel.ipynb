{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Konan_9_CnnExtModel.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1y8ITrZO4i24iuKt-UpPalpgabLI0u094","authorship_tag":"ABX9TyOzwY8zVopmo/N61ToB7Mya"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LLPcS5vQnqHU"},"outputs":[],"source":["%run /content/drive/MyDrive/Colab\\ Notebooks/Konan_8_Regularization.ipynb"]},{"cell_type":"code","source":["%run /content/drive/MyDrive/Colab\\ Notebooks/5_MLP_Dataset1234.ipynb\n","flowersDB = FlowersDataset([96, 96], [96,96,3])"],"metadata":{"id":"bfsc9PfOv2zl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CnnExtModel(CnnRegModel):\n","  macro = {}\n","\n","  def __init__(self, name, dataset, hconfigs, show_maps=False,\\\n","               l2_decay=0, l1_decay=0, dump_structure=False):\n","    self.dump_structure = dump_structure\n","    self.layer_index = 0\n","    self.layer_depth = 0\n","    self.param_count = 0\n","    super(CnnExtModel, self).__init__(name, dataset, hconfigs, show_maps,\\\n","                                      l2_decay, l1_decay)\n","    if self.dump_structure:\n","      print('Total parameter count: {}'.format(self.param_count))"],"metadata":{"id":"iL9VCt3fn7mk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_layer_param(self, input_shape, hconfig):\n","  layer_type = get_layer_type(hconfig)\n","  if layer_type in ['serial', 'parallel', 'loop', 'add', 'custom']:\n","    if self.dump_structure:\n","      dump_str = layer_type\n","      if layer_type == 'custom':\n","        name = get_conf_param(hconfig, 'name')\n","        dump_str += ' ' + name\n","      print('{:>{width}}{}'.format('', dump_str, width=self.layer_depth * 2))\n","    self.layer_depth += 1\n","  ####\n","  pm, output_shape = super(CnnExtModel, self).alloc_layer_param(input_shape, hconfig)\n","  ####\n","  if layer_type in ['serial', 'parallel', 'loop', 'add', 'custom']:\n","    self.layer_depth -= 1\n","  elif self.dump_structure:\n","    self.layer_index += 1\n","    pm_str = \"\"\n","    if layer_type == 'full':\n","      ph, pw = pm['w'].shape\n","      pm_count = np.prod(pm['w'].shape) + pm['b'].shape[0]\n","      self.param_count += pm_count\n","      pm_str = ' pm:{}x{}+{}={}'.format(ph, pw, pm['b'].shape[0], pm_count)\n","    elif layer_type == 'conv':\n","      kh, kw, xchn, ychn = pm['k'].shape\n","      pm_count = np.prod(pm['k'].shape) + pm['b'].shape[0]\n","      self.param_count += pm_count\n","      pm_str = ' pm:{}x{}x{}x{}+{}={}'.format(kh, kw, xchn, ychn,\\\n","                                              pm['b'].shape[0], pm_count)\n","    print('{:>{width}}{}: {}, {}=>{}{}'\\\n","          .format('', self.layer_index), layer_type, input_shape,\\\n","          output_shape, pm_str, width=self.layer_depth*2)\n","  return pm, output_shape\n","CnnExtModel.alloc_layer_param = cnn_ext_alloc_layer_param"],"metadata":{"id":"julZWxvAozfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_parallel_layer(self, input_shape, hconfig):\n","  pm_hiddens = []\n","  output_shape = None\n","\n","  if not isinstance(hconfig[1], dict):\n","    hconfig.insert(1, {})\n","  \n","  for bconfig in hconfig[2:]:\n","    bpm, bshape = self.alloc_layer_param(input_shape, bconfig)\n","    pm_hiddens.append(bpm)\n","    if output_shape:\n","      assert output_shape[:-1] == bshape[:-1]\n","      output_shape[-1] += bshape[-1]\n","    else:\n","      output_shape = bshape\n","  return {'pms':pm_hiddens}, output_shape\n","\n","def cnn_ext_forward_parallel_layer(self, x, hconfig, pm):\n","  bys, bauxes, bchns = [], [], []\n","  for n, bconfig in enumerate(hconfigs[2:]):\n","    by, baux = self.forward_layer(x, bconfig, pm['pms'][n])\n","    bys.append(by)\n","    bauxes.append(baux)\n","    bchns.append(by.shape[-1])\n","  y = np.concatenate(bys, axis=-1)\n","  return y, [bauxes, bchns]\n","\n","def cnn_ext_backprop_parallel_layer(self, G_y, hconfig, pm, aux):\n","  bauxes, bchns = aux\n","  bchn_from = 0\n","  G_x = 0\n","  for n, bconfig in enumerate(hconfig[2:]):\n","    bchn_to = bchn_from + bchns[n]\n","    G_y_slice = G_y[:,:,:,bchn_from:bchn_to]\n","    G_x += self.backprop_layer(G_y_slice, bconfig, pm['pms'][n], bauxes[n])\n","  return G_x\n","\n","CnnExtModel.alloc_parallel_layer = cnn_ext_alloc_layer_param\n","CnnExtModel.forward_parallel_layer = cnn_ext_forward_parallel_layer\n","CnnExtModel.backprop_parallel_layer = cnn_ext_backprop_parallel_layer"],"metadata":{"id":"VmMfySqxxDEu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_serial_layer(self, input_shape, hconfig):\n","  pm_hiddens = []\n","  prev_shape = input_shape \n","\n","  if not isinstance(hconfig[1], dict): hconfig.insert(1, {})\n","\n","  for sconfig in hconfig[2:]:\n","    pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, sconfig)\n","    pm_hiddens.append(pm_hidden)\n","  return {'pms':pm_hiddens}, prev_shape\n","\n","def cnn_ext_forward_serial_layer(self, x, hconfig, pm):\n","  hidden = x\n","  auxes = []\n","  for n, sconfig in enumerate(hconfig[2:]):\n","    hidden, aux = self.forward_layer(hidden, sconfig, pm['pms'][n])\n","    auxes.append(aux)\n","  return hidden, auxes\n","\n","def cnn_ext_backprop_serial_layer(self, G_y, hconfig, pm, aux):\n","  auxes = aux\n","  G_hidden = G_y\n","\n","  for n in reversed(range(len(hconfig[2:]))):\n","    sconfig, spm, saux = hconfig[2:][n], pm['pms'][n], auxes[n]\n","    G_hidden = self.backprop_layer(G_hidden, sconfig, spm, saux)\n","  return G_hidden\n","\n","CnnExtModel.alloc_serial_layer = cnn_ext_alloc_serial_layer\n","CnnExtModel.forward_serial_layer = cnn_ext_forward_serial_layer\n","CnnExtModel.backprop_serial_layer = cnn_ext_backprop_serial_layer"],"metadata":{"id":"xYNgN04zEc8h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_add_layer(self, input_shape, hconfig):\n","  if not isinstance(hconfig[1], dict): hconfig.insert(1, {})\n","\n","  bpm, output_shape = self.alloc_layer_param(input_shape, hconfig[2])\n","  pm_hiddens = [bpm]\n","\n","\n","  for bconfig in hconfig[3:]:\n","    bpm, bshape = self.alloc_layer_param(input_shape, bconfig)\n","    pm_hiddens.append(bpm)\n","    check_add_shapes(output_shape, bshape)\n","  \n","  if get_conf_param(hconfig, 'x', True):\n","    check_add_shapes(output_shape, input_shape)\n","  \n","  pm = {'pms':pm_hiddens}\n","\n","  for act in get_conf_param(hconfig, 'actions', ''):\n","    if act == 'B':\n","      bn_config = ['batch_noraml', {'rescale':True}]\n","      pm['bn'], _ = self.alloc_batch_normal_param(output_shape, bn_config)\n","  return pm, output_shape\n","\n","CnnExtModel.alloc_add_layer = cnn_ext_alloc_add_layer"],"metadata":{"id":"LjN_k7mDEiTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_forward_add_layer(self, x, hconfig, pm):\n","  y, baux = self.forward(x, hconfig[2], pm['pms'][0])\n","  bauxes, bchns, aux_bn = [baux], [y.shape[-1]], []\n","\n","  for n, bconfig in enumerate(hconfig[3:]):\n","    by, baux = self.forward_layer(x, bconfig, pm['pms'][n+1])\n","    y += tile_add_result(by, y.shape[-1], by.shape[-1])\n","    bauxes.append(baux)\n","    bchns.append(by.shape[-1])\n","  if get_conf_param(hconfig, 'x', True):\n","    y += tile_add_result(x, y.shape[-1], x.shape[-1])\n","  for act in get_conf_param(hconfig, 'actions', ''):\n","    if act == 'A':\n","      y = self.activate(y, hconfig)\n","    if act == 'B':\n","      y, aux_bn = self.forward_batch_normal_layer(y, None, pm['bm'])\n","  return y, [y, baux, bchns, aux_bn, x.shape]\n","CnnExtModel.forward_add_layer = cnn_ext_forward_add_layer"],"metadata":{"id":"zZbfew60OGY2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_backprop_add_layer(self, G_y, hconfig, pm, aux):\n","  y, baux, bchns, aux_bn, x_shape = aux\n","\n","  for act in reversed(get_conf_param(hconfig, 'actions', '')):\n","    if act == 'A':\n","      G_y = self.activate_derv(G_y, y, hconfig)\n","    if act == 'B':\n","      G_y = self.backprop_batch_normal_layer(G_y, None, pm['bm'], aux_bn)\n","  G_x = np.zeros(x_shape)\n","  for n, bconfig in enumerate(hconfig[2:]):\n","    G_by = merge_add_grad(G_y, G_y.shape[-1], bchns[n])\n","    G_x += self.backprop_layer(G_by, bconfig, pm['pms'][n], bauxes[n])\n","  if get_conf_param(hconfig, 'x', True):\n","    G_x += merge_add_grad(G_y, G_y.shape[-1], x_shape[-1])\n","  return G_x\n","\n","CnnExtModel.backprop_add_layer = cnn_ext_backprop_add_layer"],"metadata":{"id":"uUwDB2ODZC48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check_add_shapes(yshape, bshape):\n","  assert yshape[:-1] == bshape[:-1]\n","  assert yshape[-1] % bshape[-1] == 0\n","\n","def tile_add_result(by, ychn, bchn):\n","  if ychn == bchn: return by\n","  times = ychn // bchn\n","  return np.tile(bchn, times)\n","\n","def merge_add_grad(G_y, ychn, bchn):\n","  if ychn == bchn: return G_y\n","  times = ychn // bchn\n","  split_shape = G_y.shape[:-1] + tuple([times, bchn])\n","  return np.sum(G_y.reshape(split_shape), axis=-2)"],"metadata":{"id":"pJMmt15HaeTZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_loop_layer(self, input_shape, hconfig):\n","  pm_hiddens = []\n","  prev_shape = input_shape\n","\n","  if not isinstance(hconfig[1], dict): hconfig.insert(1, {})\n","\n","  for n in range(get_conf_param(hconfig, 'repeat', 1)):\n","    pm_hidden, prev_shape = self.alloc_layer_param(prev_shape, hconfig[2])\n","    pm_hiddens.append(pm_hidden)\n","  return {'pms':pm_hiddens}, prev_shape\n","\n","def cnn_ext_forward_loop_layer(self, x, hconfig, pm):\n","  hidden = x\n","  aux_layers = []\n","\n","  for n in range(get_conf_param(hconfig, 'repeat', 1)):\n","    hidden, aux = self.forward_layer(hidden, hconfig[2], pm['pms'][n])\n","    aux_layers.append(aux)\n","  return hidden, aux_layers\n","\n","def cnn_ext_backprop_loop_layer(self, G_y, hconfig, pm, aux):\n","  G_hidden = G_y\n","  aux_layers = aux\n","\n","  for n in reversed(range(get_conf_param(hconfig, 'repeat', 1))):\n","    G_hidden = self.backprop_layer(G_hidden, hconfig[2], pm['pms'][n], aux_layers[n])\n","  return G_hidden\n","\n","CnnExtModel.alloc_loop_layer = cnn_ext_alloc_loop_layer\n","CnnExtModel.forward_loop_layer = cnn_ext_forward_loop_layer\n","CnnExtModel.backprop_loop_layer = cnn_ext_backprop_loop_layer"],"metadata":{"id":"4W2aZHT7gwcS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_custom_layer(self, input_shape, hconfig):\n","  name = get_conf_param(hconfig, 'name')\n","  args = get_conf_param(hconfig, 'args')\n","  macro = CnnExtModel.get_macro(name, args)\n","\n","  pm_hidden, output_shape = self.alloc_layer_param(input_shape, macro)\n","  return {'pm':pm_hidden, 'macro':macro}, output_shape\n","\n","def cnn_ext_forward_custom_layer(self, x, hconfig, pm):\n","  return self.forward_layer(x, pm['macro'], pm['pm'])\n","\n","def cnn_ext_backprop_custom_layer(self, G_y, hconfig, pm, aux):\n","  return self.backprop_layer(G_y, pm['macro'], pm['pm'], aux)\n","\n","CnnExtModel.alloc_custom_layer = cnn_ext_alloc_custom_layer\n","CnnExtModel.forward_custom_layer = cnn_ext_forward_custom_layer\n","CnnExtModel.backprop_custom_layer = cnn_ext_backprop_custom_layer"],"metadata":{"id":"E6bQ6YJXj4eZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_set_macro(name, config):\n","  CnnExtModel.macro[name] = config\n","\n","def cnn_ext_get_macro(name, args):\n","  restored = copy.deepcopy(CnnExtModel.macro[name])\n","  replace_arg(restored, args)\n","  return restored\n","\n","def replace_arg(exp, args):\n","  if isinstance(exp, (list, tuple)):\n","    for n, term in enumerate(exp):\n","      if isinstance(term, str) and term[0] == '#':\n","        if term[1] == '#': exp[n] = term[1:]\n","        elif term in args: exp[n] = args[term]\n","      else:\n","        replace_arg(term, args)\n","  elif isinstance(exp, dict):\n","    for key in exp:\n","      if isinstance(exp[key], str) and exp[key][0] == '#':\n","        if exp[key][1] == '#': exp[key] = exp[key][1:]\n","        elif exp[key] in args: exp[key] = args[exp[key]]\n","      else:\n","        replace_arg(exp, args)\n","\n","CnnExtModel.set_macro = cnn_ext_set_macro\n","CnnExtModel.get_macro = cnn_ext_get_macro"],"metadata":{"id":"JLoYUExHlXoB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_conv_layer(self, input_shape, hconfig):\n","  pm, output_shape = super(CnnExtModel, self).\\\n","  alloc_conv_layer(input_shape, hconfig)\n","\n","  pm['actions'] = get_conf_param(hconfig, 'actions', 'LA')\n","  for act in pm['actions']:\n","    if act == 'L':\n","      input_shape = output_shape\n","    if act == 'B':\n","      bn_config = ['batch_normal', {'rescale':False}] # after of before linear operator, rescale is not needed.\n","      #### 'batch_normale' AND 'activate' don't change output_shape. In other words, input_shape == output_shape.\n","      pm['bn'], _ = super(CnnExtModel, self).\\\n","      alloc_batch_normal_param(input_shape, bn_config)\n","  xh, xw, xchn = input_shape\n","  ychn = get_conf_param(hconfig, 'chn')\n","  output_shape = eval_stride_shape(hconfig, True, xh, xw, ychn)\n","  return pm, output_shape\n","\n","CnnExtModel.alloc_conv_layer = cnn_ext_alloc_conv_layer"],"metadata":{"id":"qag8A6t_sI2F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_forward_conv_layer(self, x, hconfig, pm):\n","  y = x\n","  x_flat, k_flat, relu_y, axu_bn = None, None, None, None\n","  for act in pm['actions']:\n","    if act == 'L':\n","      mb_size, xh, xw, xchn = y.shape\n","      kh, kw, _, ychn = pm['k'].shape\n","      x_flat = get_ext_regions_for_conv(y, kh, kw)\n","      k_flat = pm['k'].reshape((kh*kw*xchn, ychn))\n","      conv_flat = np.matmul(x_flat, k_flat)\n","      y = conv_flat.reshape((mb_size, xh, xw, ych)) + pm['b']\n","    elif act == 'A':\n","      y = self.activate(y, hconfig)\n","      relu_y = y\n","    elif act == 'B':\n","      y, aux_bn = self.forward_batch_normal_layer(y, None, pm['bn'])\n","  y, aux_stride = stride_filter(hconfig, True, y)\n","  if self.need_maps: self.maps.append(y)\n","  return y, [x_flat, k_flat, x, relu_y, aux_bn, aux_stride]\n","\n","CnnExtModel.forward_conv_layer = cnn_ext_forward_conv_layer"],"metadata":{"id":"kiGs3nHMuOLB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_backprop_conv_layer(self, G_y, hconfig, pm, aux):\n","  x_flat, k_flat, x, relu_y, aux_bn, aux_stride = aux\n","\n","  G_x = stride_filter_derv(hconfig, True, G_y, aux_stride)\n","  for act in reversed(pm['actions']):\n","    if act == 'L':\n","      kh, kw, xchn, ychn = pm['k'].shape\n","      mb_size, xh, xw, _ = G_x.shape\n","\n","      G_conv_flat = G_x.reshape((mb_size * xh * xw, ychn))\n","      G_x_flat = np.matmul(G_conv_flat, k_flat.T)\n","      G_k_flat = np.matmul(x_flat.T, G_conv_flat)\n","      G_bias = np.sum(G_conv_flat, axis=0)\n","\n","      G_k = G_k_flat.reshape((kh, kw, xchn, ychn))\n","      G_x = undo_ext_regions_for_conv(G_x_flat, x, kh, kw)\n","\n","      self.update_param(pm, 'k', G_kernel)\n","      self.update_param(pm, 'b', G_bias)\n","    elif act == 'A':\n","      G_x = self.activate_derv(G_x, relu_y, hconfig)\n","    elif act == 'B':\n","      G_x = self.backprop_batch_normal_layer(G_x, None, pm['bn'], aux_bn)\n","  return G_x\n","CnnExtModel.backprop_conv_layer = cnn_ext_backprop_conv_layer"],"metadata":{"id":"Br8i1IFG4hY3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_max_layer(self, input_shape, hconfig):\n","  xh, xw, ychn = input_shape\n","  output_shape = eval_stride_shape(hconfig, False, xh, xw, ychn)\n","  return None, output_shape\n","\n","CnnExtModel.alloc_max_layer = cnn_ext_alloc_max_layer"],"metadata":{"id":"RfsvDCCB6oiF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_forward_max_layer(self, x, hconfig, pm):\n","  mb_size, xh, xw, xchn = x.shape\n","  \n","  sh, sw = get_conf_param_2d(hconfig, 'stride', [1, 1])\n","  kh, kw = get_conf_param_2d(hconfig, 'ksize', [sh ,sw])\n","  padding = get_conf_param(hconfig, 'padding', 'SAME')\n","\n","  if [sh, sw] == [kh, kw] and xh % sh == 0 and xw % sw == 0 and padding == 'SAME':\n","    return super(CnnExtModel, self).forward_max_layer(x, hconfig, pm)\n","  x_flat = get_ext_regions(x, kh, kw, -np.inf)\n","  x_flat = x_flat.transpose((2, 5, 0, 1, 3, 4))\n","  x_flat = x_flat.reshape((mb_size * xchn * xh * xw, kh * kw))\n","  max_idx = np.argmax(x_flat, axis=1)\n","  y = x_flat[np.arange(x_flat.shape[0]), max_idx]\n","  y = y.reshape((mb_size, xchn, xh, xw))\n","  y = y.transpose((0, 2, 3, 1))\n","\n","  y, aux_stride = stride_filter(hconfig, False, y)\n","\n","  if self.need_maps: self.maps.append(y)\n","\n","  return y, [x.shape, kh, kw, sh, sw, padding, max_idx, aux_stride]\n","\n","CnnExtModel.forward_max_layer = cnn_ext_forward_max_layer"],"metadata":{"id":"b_aT3QHBPWZx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_backprop_max_layer(self, G_y, hconfig, pm, aux):\n","  if not isinstance(aux, list):\n","    return super(CnnExtModel, self).backprop_max_layer(G_y, hconfig, pm, aux)\n","  x_shape, kh, kw, sh, sw, padding, max_idx, aux_stride = aux\n","  mb_size, xh, xw, xchn = x_shape\n","\n","  G_y = stride_filter_derv(hconfig, False, G_y, aux_stride)\n","  \n","  G_y = G_y.transpose((0, 3, 1, 2))\n","  G_y = G_y.flatten()\n","\n","  G_x_flat = np.zeros((mb_size * xchn * xh * xw, kh * kw))\n","  G_x_flat[np.arange(G_x_flat.shape[0]), max_idx] = G_y\n","\n","  G_x_flat = G_x_flat.reshape(mb_size, xchn, xh, xw, kh, kw)\n","  G_x_flat = G_x_flat.tranpose((2, 3, 0, 4, 5, 1))\n","  G_x = undo_ext_regions(G_x_flat, kh, kw)\n","  return G_x\n","CnnExtModel.backprop_max_layer = cnn_ext_backprop_max_layer"],"metadata":{"id":"UXC4aYt4YyF_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_alloc_avg_layer(self, input_shape, hconfig):\n","  xh, xw, xchn = input_shape\n","  sh, sw = get_conf_param_2d(hconfig, 'stride', [1, 1])\n","  kh, kw = get_conf_param_2d(hconfig, 'ksize', [sh, sw])\n","  padding = get_conf_param(hconfig, 'padding', 'SAME')\n","\n","  if [sh, sw] == [kh, kw] and xh % sh == 0 and xw % sw == 0 \\\n","  and padding == 'SAME':\n","    return super(CnnExtModel, self).alloc_avg_layer(input_shape, hconfig)\n","  \n","  one_mask = np.ones((1, xh, xw, xchn))\n","  m_flat = get_ext_regions(one_mask, kh, kw, 0) # xh, xw, 1, kh, kw, xchn\n","  m_flat = m_flat.transpose((2, 5, 0, 1, 3, 4))\n","  m_flat = m_flat.reshape(1*xchn*xh*xw, kh*kw)\n","\n","  mask = np.sum(m_flat, axis=1)\n","\n","  output_shape = eval_stride_shape(hconfig, False, xh, xw, xchn)\n","\n","  return {'mask':mask}, output_shape\n","\n","CnnExtModel.alloc_avg_layer = cnn_ext_alloc_avg_layer"],"metadata":{"id":"T6Qlh6XiY-zE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_forward_avg_layer(self, x, hconfig, pm):\n","  mb_size, xh, xw, xchn = x.shape\n","  sh, sw = get_conf_param_2d(hconfig, 'stride', [1, 1])\n","  kh, kw = get_conf_param_2d(hconfig, 'ksize', [sh, sw])\n","  padding = get_conf_param(hconfig, 'padding', 'SAME')\n","\n","  if [sh, sw] == [kh, kw] and xh % sh == 0 and xw % sw == 0\\\n","  and padding == 'SAME':\n","    return super(CnnExtModel, self).forward_avg_layer(x, hconfig, pm)\n","  \n","  x_flat = get_ext_regions(x, kh, kw, 0) # xh, xw, mb_size, kh, kw, xchn\n","  x_flat = x_flat.transpose((2, 5, 0, 1, 3, 4)) # mb_size, xchn, xh, xw, kh, kw\n","  x_flat = x_flat.reshape(mb_size * xchn * xh * xw, kh * kw)\n","\n","  adder = np.sum(x_flat, axis=1)  #mb_size * xchn * xh * xw\n","  y = adder.reshape((mb_size , -1)) / pm['mask']\n","  y = y.reshape((mb_size, xchn, xh, xw))\n","  y = y.transpose((0, 2, 3, 1))\n","\n","  y, aux_stride = stride_filter(hconfig, False, y)\n","  if self.need_maps: self.maps.append(y)\n","\n","  return y, [x.shape, kh, kw, sh, sw, padding, aux_stride]\n","\n","CnnExtModel.forward_avg_layer = cnn_ext_forward_avg_layer"],"metadata":{"id":"pijPp3UEqI9B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cnn_ext_backprop_avg_layer(self, G_y, hconfig, pm, aux):\n","  if not isinstance(aux, list):\n","    return super(CnnExtModel, self).backprop_avg_layer(G_y, hconfig, pm, aux)\n","  \n","  x_shape, kh, kw, sh, sw, padding, aux_stride = aux\n","  mb_size, xh, xw, xchn = x_shape\n","\n","  G_y = stride_filter_derv(hconfig, False, G_y, aux_stride)\n","  G_y = G_y.transpose((0, 3, 1, 2))\n","  G_y = G_y.flatten()\n","\n","  G_adder = G_y.reshape((mb_size, -1)) / pm['mask']\n","  G_x_flat = np.tile(G_adder, (kh*kw, 1))\n","  G_x_flat = G_x_flat.reshape(mb_size, xchn, xh, xw, kh, kw)\n","  G_x_flat = G_x_flat.transpose((2, 3, 0, 4, 5, 1))\n","  G_x = undo_ext_regions(G_x_flat, kh, kw)\n","  return G_x\n","\n","CnnExtModel.backprop_avg_layer = cnn_ext_backprop_avg_layer"],"metadata":{"id":"QAxrFBsas2Fb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"hHNBbGtlIQsj"},"execution_count":null,"outputs":[]}]}