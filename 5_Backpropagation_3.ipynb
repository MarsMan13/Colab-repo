{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"PzVML14vagCo","executionInfo":{"status":"ok","timestamp":1639991842622,"user_tz":-540,"elapsed":2,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"nbUXlMNlanDJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639991857689,"user_tz":-540,"elapsed":15069,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}},"outputId":"f24aba50-a668-4c1b-a345-26e1f28d1d1f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","%cd drive/MyDrive/Colab\\ Notebooks"]},{"cell_type":"code","source":["from dataset.mnist import load_mnist\n","from sklearn.datasets import fetch_openml\n","mnist = fetch_openml(\"mnist_784\")"],"metadata":{"id":"XSXhLq4_7vjz","executionInfo":{"status":"ok","timestamp":1639992023747,"user_tz":-540,"elapsed":48524,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CvCQ_bqU_6hn","executionInfo":{"status":"ok","timestamp":1639991914883,"user_tz":-540,"elapsed":10,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"outputs":[],"source":["def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))    \n","\n","\n","def sigmoid_grad(x):\n","    return (1.0 - sigmoid(x)) * sigmoid(x)\n","\n","def softmax(x):\n","    if x.ndim == 2:\n","        x = x.T\n","        x = x - np.max(x, axis=0)\n","        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n","        return y.T \n","\n","    x = x - np.max(x) # 오버플로 대책\n","    return np.exp(x) / np.sum(np.exp(x))\n","\n","def cross_entropy_error(y, t):\n","    if y.ndim == 1:\n","        t = t.reshape(1, t.size)\n","        y = y.reshape(1, y.size)\n","        \n","    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n","    if t.size == y.size:\n","        t = t.argmax(axis=1)\n","             \n","    batch_size = y.shape[0]\n","    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n","\n","\n","def softmax_loss(X, t):\n","    y = softmax(X)\n","    return cross_entropy_error(y, t)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"JkSuB1qP_gEq","executionInfo":{"status":"ok","timestamp":1639991914883,"user_tz":-540,"elapsed":10,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"outputs":[],"source":["class Relu:\n","    def __init__(self):\n","        self.mask = None\n","\n","    def forward(self, x):\n","        self.mask = (x <= 0)\n","        out = x.copy()\n","        out[self.mask] = 0\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dout[self.mask] = 0\n","        dx = dout\n","\n","        return dx\n","\n","\n","class Sigmoid:\n","    def __init__(self):\n","        self.out = None\n","\n","    def forward(self, x):\n","        out = sigmoid(x)\n","        self.out = out\n","        return out\n","\n","    def backward(self, dout):\n","        dx = dout * (1.0 - self.out) * self.out\n","\n","        return dx\n","\n","\n","class Affine:\n","    def __init__(self, W, b):\n","        self.W = W\n","        self.b = b\n","        \n","        self.x = None\n","        self.original_x_shape = None\n","        # 가중치와 편향 매개변수의 미분\n","        self.dW = None\n","        self.db = None\n","\n","    def forward(self, x):\n","        # 텐서 대응\n","        self.original_x_shape = x.shape\n","        x = x.reshape(x.shape[0], -1)\n","        self.x = x\n","\n","        out = np.dot(self.x, self.W) + self.b\n","\n","        return out\n","\n","    def backward(self, dout):\n","        dx = np.dot(dout, self.W.T)\n","        self.dW = np.dot(self.x.T, dout)\n","        self.db = np.sum(dout, axis=0)\n","        \n","        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n","        return dx\n","\n","\n","class SoftmaxWithLoss:\n","    def __init__(self):\n","        self.loss = None # 손실함수\n","        self.y = None    # softmax의 출력\n","        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n","        \n","    def forward(self, x, t):\n","        self.t = t\n","        self.y = softmax(x)\n","        self.loss = cross_entropy_error(self.y, self.t)\n","        \n","        return self.loss\n","\n","    def backward(self, dout=1):\n","        batch_size = self.t.shape[0]\n","        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n","            dx = (self.y - self.t) / batch_size\n","        else:\n","            dx = self.y.copy()\n","            dx[np.arange(batch_size), self.t] -= 1\n","            dx = dx / batch_size\n","        \n","        return dx\n","\n","def numerical_gradient(f, x):\n","    h = 1e-4 # 0.0001\n","    grad = np.zeros_like(x)\n","    \n","    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n","    while not it.finished:\n","        idx = it.multi_index\n","        tmp_val = x[idx]\n","        x[idx] = float(tmp_val) + h\n","        fxh1 = f(x) # f(x+h)\n","        \n","        x[idx] = tmp_val - h \n","        fxh2 = f(x) # f(x-h)\n","        grad[idx] = (fxh1 - fxh2) / (2*h)\n","        \n","        x[idx] = tmp_val # 값 복원\n","        it.iternext()   \n","        \n","    return grad"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"YP854Fesaqt-","executionInfo":{"status":"ok","timestamp":1639991914885,"user_tz":-540,"elapsed":11,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"outputs":[],"source":["from collections import OrderedDict\n","\n","class TwoLayerNet:\n","\n","    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n","        # 가중치 초기화\n","        self.params = {}\n","        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n","        self.params['b1'] = np.zeros(hidden_size)\n","        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n","        self.params['b2'] = np.zeros(output_size)\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n","        self.layers['Relu1'] = Relu()\n","        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n","        self.layers['Relu2'] = Relu()\n","\n","        self.lastLayer = SoftmaxWithLoss()\n","        \n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","        \n","        return x\n","        \n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, t)\n","    \n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","        \n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","        \n","    # x : 입력 데이터, t : 정답 레이블\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","        \n","        grads = {}\n","        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n","        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n","        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n","        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n","        \n","        return grads\n","        \n","    def gradient(self, x, t):\n","        # forward\n","        self.loss(x, t)\n","\n","        # backward\n","        dout = 1\n","        dout = self.lastLayer.backward(dout)\n","        \n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n","        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n","\n","        return grads"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iN9FErXFAQ9h","executionInfo":{"status":"ok","timestamp":1639991914885,"user_tz":-540,"elapsed":11,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"outputs":[],"source":["#(X_train, t_train), (X_test, t_test) = load_mnist(normalize=True, one_hot_label=True)"]},{"cell_type":"code","source":["mnist_X, mnist_y = mnist.data.values, mnist.target.values\n","mnist_y = mnist_y.astype(np.int)\n","y_ = np.zeros((mnist_y.shape[0], 10), dtype=np.int)\n","y_[np.arange(y_.shape[0]), mnist_y] = 1"],"metadata":{"id":"XorMXYSsFJM5","executionInfo":{"status":"ok","timestamp":1639992067838,"user_tz":-540,"elapsed":220,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import StandardScaler\n","\n","mnist_X_scaled = StandardScaler().fit_transform(mnist_X)"],"metadata":{"id":"hhd2LDRoG5D3","executionInfo":{"status":"ok","timestamp":1639992072354,"user_tz":-540,"elapsed":1002,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, t_train, t_test = train_test_split(mnist_X_scaled, y_, test_size=0.1)"],"metadata":{"id":"xOtNQ_sjGT9h","executionInfo":{"status":"ok","timestamp":1639992075391,"user_tz":-540,"elapsed":1868,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40228,"status":"ok","timestamp":1639992116510,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"},"user_tz":-540},"id":"SL25Xg3na2Me","outputId":"f0782e7c-c4ad-4ae7-87dc-c21e8be61b96"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.17714285714285713 0.17514285714285716\n","0.9405396825396826 0.9351428571428572\n","0.9557301587301588 0.9461428571428572\n","0.9668571428571429 0.958\n","0.9726507936507937 0.9612857142857143\n","0.9766031746031746 0.964\n","0.9797619047619047 0.964\n","0.9824444444444445 0.9657142857142857\n","0.984920634920635 0.9675714285714285\n","0.9862063492063492 0.9667142857142857\n","0.9873015873015873 0.9681428571428572\n","0.9895238095238095 0.968\n","0.989984126984127 0.9658571428571429\n","0.9914603174603175 0.9685714285714285\n","0.9921587301587301 0.9687142857142857\n","0.9934126984126984 0.9697142857142858\n"]}],"source":["network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n","\n","iters_num = 10000\n","train_size = X_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","iter_per_epoch = max(train_size / batch_size, 1)\n","\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    x_batch = X_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","    \n","    # 기울기 계산\n","    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n","    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n","    \n","    # 갱신\n","    for key in ('W1', 'b1', 'W2', 'b2'):\n","        network.params[key] -= learning_rate * grad[key]\n","    \n","    loss = network.loss(x_batch, t_batch)\n","    train_loss_list.append(loss)\n","    \n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(X_train, t_train)\n","        test_acc = network.accuracy(X_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(train_acc, test_acc)"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"kVUt0nuOAZcr","executionInfo":{"status":"ok","timestamp":1639992116511,"user_tz":-540,"elapsed":14,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"outputs":[],"source":["from collections import OrderedDict\n","\n","class LayerNet:\n","\n","    def __init__(self, layer_size=3, layer_infos=None, weight_init_std = 0.01):\n","        self.layer_size = layer_size\n","        self.layer_infos = layer_infos\n","\n","        # 가중치 초기화\n","        self.params = {}\n","        for i in range(self.layer_size):\n","          self.params['W'+str(i+1)] = weight_init_std * np.random.randn(layer_infos[i], layer_infos[i+1])\n","          self.params['b'+str(i+1)] = np.zeros(layer_infos[i+1])\n","\n","        # 계층 생성\n","        self.layers = OrderedDict()\n","        for i in range(self.layer_size):\n","          self.layers['Affine'+str(i+1)] = Affine(self.params['W'+str(i+1)], self.params['b'+str(i+1)])\n","          self.layers['Relu'+str(i+1)] = Relu()\n","        self.lastLayer = SoftmaxWithLoss()\n","        \n","    def predict(self, x):\n","        for layer in self.layers.values():\n","            x = layer.forward(x)\n","        return x\n","        \n","    # x : 입력 데이터, t : 정답 레이블\n","    def loss(self, x, t):\n","        y = self.predict(x)\n","        return self.lastLayer.forward(y, t)\n","    \n","    def accuracy(self, x, t):\n","        y = self.predict(x)\n","        y = np.argmax(y, axis=1)\n","        if t.ndim != 1 : t = np.argmax(t, axis=1)\n","        \n","        accuracy = np.sum(y == t) / float(x.shape[0])\n","        return accuracy\n","        \n","    # x : 입력 데이터, t : 정답 레이블\n","    def numerical_gradient(self, x, t):\n","        loss_W = lambda W: self.loss(x, t)\n","        \n","        grads = {}\n","        for i in range(1, self.layer_size+1):\n","          grads['W'+str(i)] = numerical_gradient(loss_W, self.params['W'+str(i)])\n","          grads['b'+str(i)] = numerical_gradient(loss_W, self.params['b'+str(i)])\n","        return grads\n","        \n","    def gradient(self, x, t):\n","        # forward\n","        self.loss(x, t)\n","\n","        # backward\n","        dout = 1\n","        dout = self.lastLayer.backward(dout)\n","        \n","        layers = list(self.layers.values())\n","        layers.reverse()\n","        for layer in layers:\n","            dout = layer.backward(dout)\n","\n","        # 결과 저장\n","        grads = {}\n","        for i in range(self.layer_size):\n","          grads['W'+str(i+1)], grads['b'+str(i+1)] = self.layers['Affine'+str(i+1)].dW, self.layers['Affine'+str(i+1)].db\n","\n","        return grads"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7pKXIbCDqBh","outputId":"16ab68b0-6cc4-4e28-f124-867f97032c13","executionInfo":{"status":"ok","timestamp":1639992157078,"user_tz":-540,"elapsed":40579,"user":{"displayName":"G.M. C","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01123269045867249031"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["630\n","0.1179047619047619 0.11828571428571429\n","0.9385079365079365 0.9318571428571428\n","0.9558253968253968 0.949\n","0.9658888888888889 0.9561428571428572\n","0.9718888888888889 0.9607142857142857\n","0.9757619047619047 0.961\n","0.9788571428571429 0.9634285714285714\n","0.9821587301587301 0.965\n","0.9844285714285714 0.965\n","0.9865873015873016 0.9675714285714285\n","0.9872380952380952 0.9665714285714285\n","0.9892380952380952 0.9685714285714285\n","0.9906349206349206 0.9687142857142857\n","0.9914444444444445 0.969\n","0.9924603174603175 0.968\n","0.9934603174603175 0.9694285714285714\n"]}],"source":["network = LayerNet(layer_size=2, layer_infos=[784, 50, 10])\n","\n","iters_num = 10000\n","train_size = X_train.shape[0]\n","batch_size = 100\n","learning_rate = 0.1\n","\n","train_loss_list = []\n","train_acc_list = []\n","test_acc_list = []\n","\n","iter_per_epoch = max(train_size // batch_size, 1)\n","print(iter_per_epoch)\n","for i in range(iters_num):\n","    batch_mask = np.random.choice(train_size, batch_size)\n","    X_batch = X_train[batch_mask]\n","    t_batch = t_train[batch_mask]\n","    \n","    grad = network.gradient(X_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n","    \n","    # 갱신\n","    for ii in range(network.layer_size):\n","      network.params['W'+str(ii+1)] -= learning_rate * grad['W'+str(ii+1)]\n","      network.params['b'+str(ii+1)] -= learning_rate * grad['b'+str(ii+1)]\n","    \n","    loss = network.loss(X_batch, t_batch)\n","    train_loss_list.append(loss)\n","\n","    if i % iter_per_epoch == 0:\n","        train_acc = network.accuracy(X_train, t_train)\n","        test_acc = network.accuracy(X_test, t_test)\n","        train_acc_list.append(train_acc)\n","        test_acc_list.append(test_acc)\n","        print(train_acc, test_acc)"]},{"cell_type":"code","source":[""],"metadata":{"id":"hsXxj10OMXDZ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"name":"5_Backpropagation_3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMsRpz1JIDaalU9Q1ZOTN1z"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}